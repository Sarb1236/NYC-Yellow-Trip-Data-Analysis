{"cells":[{"cell_type":"code","source":["# Azure Blob Storage info (public access â€” no SAS token needed)\n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"nyctlc\"\n","blob_relative_path = \"yellow\"\n","blob_sas_token = \"\"  # Leave blank for public read access\n","\n","# Construct WASBS path\n","wasbs_path = f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/{blob_relative_path}\"\n","\n","# Set Spark config to allow access\n","spark.conf.set(\n","    f\"fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net\",\n","    blob_sas_token\n",")\n","\n","print(\"Remote blob path:\", wasbs_path)\n","\n","# Step 1: Read Parquet from Azure Blob\n","df = spark.read.parquet(wasbs_path)\n","\n","# Step 2: Add 'year' column for partitioning\n","from pyspark.sql.functions import year\n","\n","df = df.withColumn(\"year\", year(\"tpepPickupDateTime\"))\n","\n","# Step 3: Save to Microsoft Fabric Lakehouse as a Delta table\n","# Make sure your notebook is attached to a Lakehouse destination!\n","df.write \\\n","  .format(\"delta\") \\\n","  .mode(\"overwrite\") \\\n","  .partitionBy(\"year\") \\\n","  .save(\"Tables/yellow_tripdata\")  # <- Fabric path for Lakehouse tables\n","\n","print(\"Data written to Lakehouse table: yellow_tripdata\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"45c55d68-ae2e-4e47-8e79-53d2c71d4922","normalized_state":"finished","queued_time":"2025-07-03T02:18:43.3514369Z","session_start_time":null,"execution_start_time":"2025-07-03T02:18:43.3526878Z","execution_finish_time":"2025-07-03T02:26:31.8955118Z","parent_msg_id":"8b21f048-a403-4cef-90a9-bb128a371666"},"text/plain":"StatementMeta(, 45c55d68-ae2e-4e47-8e79-53d2c71d4922, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ðŸ“¦ Remote blob path: wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow\nâœ… Data written to Lakehouse table: yellow_tripdata\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"36156a35-ae66-408a-9cc2-c3d49eb0344a"},{"cell_type":"code","source":["\n","#Day level agg table, we can compare it with monthly compact agg table\n","%%sql\n","CREATE OR REPLACE TABLE fact_vendor_daily_activity AS\n","SELECT\n","  vendorID,\n","  DATE_TRUNC('month', tpepPickupDateTime) AS activity_month,\n","  CAST(tpepPickupDateTime AS DATE) AS trip_date,\n","  COUNT(*) AS trip_count,\n","  SUM(passengerCount) AS total_passengerCount,\n","  SUM(tripDistance) AS total_tripDistance,\n","  SUM(fareAmount) AS total_fareAmount,\n","  SUM(extra) AS total_extra,\n","  SUM(mtaTax) AS total_mtaTax,\n","  SUM(tipAmount) AS total_tipAmount,\n","  SUM(tollsAmount) AS total_tollsAmount,\n","  SUM(totalAmount) AS total_totalAmount\n","FROM yellow_tripdata\n","GROUP BY vendorID, DATE_TRUNC('month', tpepPickupDateTime), CAST(tpepPickupDateTime AS DATE)\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":2,"statement_ids":[2],"state":"finished","livy_statement_state":"available","session_id":"e9074ce9-7d8f-444e-85f3-0607c9a8c6ef","normalized_state":"finished","queued_time":"2025-07-03T21:55:15.0697316Z","session_start_time":"2025-07-03T21:55:15.0707303Z","execution_start_time":"2025-07-03T21:55:23.2341728Z","execution_finish_time":"2025-07-03T21:58:56.417232Z","parent_msg_id":"3d4c6db7-7ce8-4633-9b00-870d6ccaa74f"},"text/plain":"StatementMeta(, e9074ce9-7d8f-444e-85f3-0607c9a8c6ef, 2, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":1,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"115ab58a-e381-404a-8228-c80fd090a2cf"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"ffbee4e6-354a-44b1-ae5e-1f8f144c855e"}],"default_lakehouse":"ffbee4e6-354a-44b1-ae5e-1f8f144c855e","default_lakehouse_name":"Lakehouse_1","default_lakehouse_workspace_id":"a18de6dc-142b-4a78-a70f-f658262e8be7"}}},"nbformat":4,"nbformat_minor":5}